The Rocky Mountain Biological Lab (RMBL) located in the Gunnison Valley does annual and semi-annual drone captures of open space in the greater valley–Gunnison, Crested Butte, and others. They transform these drone captures into orthomosaic maps, and use these maps for a variety of causes. One of which is identifying snowpack. The old way to do this is have agents in the field manually note where and where there is not snow, but this takes a long time and may not be too accurate. Using the drone maps, there is a potential to streamline and even automate the identification process through bitmaps. These bitmaps are white where there is snow and black where there isn’t. And better yet, we can automate these bitmaps through the use of Machine Learning (ML). This paper focuses on exploration of the data, modification of the data, and first iterations of ML algorithms. It then follows up with further research and potential second and further iterations of the current stage.
The data presented by RMBL requires several modifications to get the best result out of any ML algorithm. One important piece is data normalization. This handles many issues faced by neural networks, like how they may prioritize features that are of a larger scale than others. Normalization also allows for quicker training by keeping reasonable weights and biases, and preventing vanishing and exploding gradients. As an image is split into three channels–red, green, and blue–we can normalize each separately. The following is a comparison of each channel in each input image, plotting the means and standard deviations.

The largest necessity in training a neural network is the training data. These snow bitmaps must exist in order to train a neural network to generate snow bitmaps. Luckily, RMBL has provided over 100 images and their corresponding bitmaps that have been manually identified for training purposes. However, while analyzing the data, I discovered that the labels provided by RMBL have values as low as -1e30 in them. The following is an example of a label, where values below zero have been colored gray.

>>IMAGE>>image1.png>>Image 1

These low values appear to tell where the edges of the classification are. Thus, I have treated them as NAN values, and have discluded them from my loss function and accuracy calculations.
Choosing the correct hyperparameters for this network is another challenge. While every optimizer has its purpose, I choose the Adam optimizer for its all around effectiveness. I paired this optimizer with a standard learning rate of 0.001. I choose batch sizes of five for this first iteration based on the fact that larger batch sizes lead to more inaccurate test results.
The first network I created is a more basic Convolutional Neural Network (CNN). These slide trained kernels across images, modifying them to a desired output. Since I am going from an image to an image, I am first testing a fully convolutional neural network–one which has no linear or traditional layers. The network starts at three channels–for red, green, and blue–and goes up to 128 channels. It then reduces down to one channel, has a sigmoid activation function applied, and is then analyzed with a binary cross entropy (BCE) loss function. This loss function is best when there are only two desired outcomes.

I explored additional network options such as a U-Net. The U-Net architecture involves shrinking the image down to identify larger features in the image and then expanding it back to the output size. To preserve finer details, copies of the image are sent across the network and re added to its later forms.

I additionally proposed a custom built network for the project. This network is inspired by U-Net, but contains a separate modification to maintain detail in the image. The image is shrunk down to a smaller size like U-Net. Then, the image is flattened into a linear network with several fully connected layers. These layers output values that are added and multiplied to every pixel in the value’s corresponding channel.

>>IMAGE>>image13.png>>A visual representation of the custom network presented above.

Before training these additional networks, I noticed an anomaly between the training data and labels. They were not properly aligned, leading many points to being misclassified. After bringing this issue to RMBL, they provided shapefiles used to clip the training data to match the shape of the labels. This clipping can be easily done using the python plugins rasterio and fiona and I followed the procedure outlined by Mapbox. The improvement created by this change is immense, bringing an average of around 50% to 98% on the basic convolutional neural network (CNN). The U-Net and custom network both achieved incredible outcomes as well, as outlined in the following histogram.

>>IMAGE>>image14.png>>A histogram comparing the performances of the CNN (Model 1), the U-Net (Model 2), and the custom network (Model 3) on the 75 classifications in the training data.

All three models achieved accuracies above 98%, with the custom model achieving an average accuracy of 99.08% on the training dataset. The accuracies did not drop much when analyzed with the testing data. This indicates that the networks did not overfit the training data. This additionally means that the model can easily extrapolate to new sites, as some of the testing data included sites not trained upon.

>>IMAGE>>image12.png>>A histogram comparing the performances of the CNN (Model 1), the U-Net (Model 2), and the custom network (Model 3) on the 41 classifications in the testing data.

An interesting trend occurs in the testing results. Model 3 is now performing the worst while Model 1 is performing the best. Model 1’s accuracy fell from 98.128% to 98.100% which means it generalizes the best of the three models. Model 3 has an outlier around 50% which I plan to analyze to see why that is the case.

>>IMAGE>>image8.png>>A side by side comparison of the prediction (left) that the network created and the label (right) that is the true identification.

This trio of networks all offer a great contribution to RMBL and their snowpack identification. Model 1 in particular generalizes extremely well to new sites and images so I have exported that model into a deployable python file. Some of the next steps include analyzing where Model 3 is failing in the testing data. I believe that with more fine tuning this model has the potential to perform the best in both categories. I also wish to compare performance between normalized and original images. While normalization can be extremely useful, this may be a setting where it is inhibiting the performance of the models. More hyperparameter tuning can be done as well. I plan to use Bayesian Optimization to tune parameters such as batch size, learning rate, and the number of hidden layers. The code can be accessed with instructions in my GitHub:

>>LINK>>https://github.com/gmoyer/RMBL-Snowpack>>Snowpack Identification Project
